## 强化学习中一些基本概念：

**人类反馈强化学习(Reinforcement Learning from Human Feedback, RLHF)**：
核心思想:让LLM的输出能够符合人类偏好(即人类反馈),主要通过人类偏好数据集(例如针对一个response,会
有人类手工标注对其打分)训练一个奖励模型,通过奖励模型计算奖励,以此来判断LLM的输出是否符合人类偏好。

早期的RLHF主要涉及到四个模型:
Actor:主模型,即需要更新的策略模型,负责输入prompt,输出token
Ref(reference model) 微调前的模型,负责防止actor模型在更新过程中偏离原模型过大
RM(reward model): 奖励模型,提前训练好的模型,负责为actor的输出打分,主要目标为是否符合人类偏好
Critic (也叫value model):评估模型,负责根据状态来估计按照当前策略进行能够带来多少期望回报,进而计算优势,优势用于更新actor模型,而critic本身也会更新,更新目标是更准确地估计期望回报。
备注：
ref模型，和rm模型都是不会更新，更新的只有critic模型，ref模型（主要防止偏离），现在考虑去掉，因为大模型能力变强


**利用 (Exploitation)**: 选择当前已知能带来高奖励的行动。即“吃老本”，利用现有知识最大化短期收益。
**探索 (Exploration)**: 尝试新的或不确定奖励的行动，以收集更多信息，可能发现长期收益更高的策略。即“冒险”，为了长期利益牺牲短期收益。

**熵 (Entropy)**： 对于一个概率分布，熵越高，表示分布越均匀，随机性越大，越不确定；熵越低，表示分布越集中（某个行动概率接近1），确定性越强。

公式: 对于离散策略 π(a|s)，其熵 H 的计算公式为：H(π(·|s)) = -Σ π(a|s) * log π(a|s)
在RL中的作用:
鼓励探索: 在损失函数中加入熵奖励 (Entropy Bonus)，即 +β * H(π(·|s))，可以鼓励策略保持一定的随机性，防止其过早地收敛到一个局部最优的确定性策略，从而探索更多可能的行为。

平衡方法:
ε-贪婪 (ε-Greedy): 以 (1-ε) 的概率选择当前最优行动（利用），以 ε 的概率随机选择行动（探索）。
上置信界算法 (UCB): 为每个行动的期望奖励增加一个不确定性bonus，优先选择期望奖励高或不确定性大的行动。
基于策略的方法 (Policy-based Methods): 直接学习一个随机策略（如输出行动的概率分布），策略本身的随机性就天然地提供了探索。熵是衡量这种随机性的关键指标

**优势函数** (Advantage Function) A(s, a) = Q(s, a) - V(s)：
使用优势函数而不是纯奖励Q，可以降低方差，使学习更稳定。

**Lora**：核心是低秩适应，通过引入少量可训练参数高效微调模型；
**QLoRA**：在LoRA的基础上，通过对预训练模型进行量化压缩，显著降低了微调对显存的需求

### Reward奖励获取方式分别是什么
1. 人工标注（Human Annotation） 优点是符合人类偏好，缺点是成本高
2. 模型基于规则/启发式生成（Rule-based/Heuristic Reward）
3. 奖励模型（Reward Model - RM） 首先用1人工标注训练一个单独的奖励模型
4. 对抗训练（Adversarial Training）训练一个判别器

## 强化学习中用到算法：

**PPO原理**：
核心思想: 在更新策略时，避免步长太长（更新幅度太大）导致策略崩溃（性能急剧下降）

关键机制:
比率 (Ratio): r(θ) = π_θ(a|s) / π_θ_old(a|s)。新策略概率除以旧策略概率。
裁剪 (Clipping): PPO的损失函数会裁剪这个比率，将其限制在区间 [1 - ε, 1 + ε] 内（ε是一个小超参数，如0.1或0.2）。这确保了每次更新都是“小幅且安全的”。
目标函数: L = min( r(θ) * A, clip(r(θ), 1-ε, 1+ε) * A )。其中A是优势函数（见下文）。通过取最小值，算法忽略了那些会导致策略发生巨大变化且对性能有负面影响的更新。

**GRPO (Group Relative Policy Optimization)**
核心思想: 在有多个人类标注者偏好（可能存在分歧）的场景下，不学习一个单一的奖励模型，而是学习一个奖励模型组。

原理: 训练多个奖励模型 {RM_i}，每个代表不同的人类偏好子群体。在RL优化时，策略的更新要考虑到整个模型组的共识，而不是单个模型。例如，目标函数可能是最小化策略相对于所有奖励模型的最差情况性能（类似于鲁棒优化），或者是根据模型组的平均奖励进行优化。这提高了策略的鲁棒性和公平性。

**DAPO (Distributional Advantage Policy Optimization)**
核心思想: 标准PPO使用优势函数的点估计（一个值），DAPO则使用优势函数的完整分布。

原理: 它建模优势函数的不确定性。通过考虑优势的分布，智能体可以更智能地平衡探索与利用：对于那些优势估计不确定性很高的行动，即使其均值略低，也可能值得探索。这可以带来更稳定、更高效的学习。

**VAPO (Value-Augmented Policy Optimization)**

原理 : 可能通过增加价值函数损失的权重，或使用价值函数来更好地估计优势，以减少方差，提升PPO的稳定性。

训练框架###
verl框架：verl 是一个专为大语言模型（LLM） 设计的强化学习（RL）训练框架

核心创新：HybridFlow 与 3D-HybridEngine
verl 的核心是其 HybridFlow 架构和 3D-HybridEngine。

HybridFlow 通过混合编程模型解耦了RLHF训练中的控制流（高层逻辑，如多个模型角色的交互）和计算流（底层执行，如单个模型的前向/反向传播）。这使得算法工程师可以更专注于高层逻辑的设计，而无需过度纠缠于底层的分布式执行细节。


**工作流程（以PPO为例）：**
一个典型的RL训练循环（例如PPO）在verl中大致包含以下步骤：
生成 (Generation): Actor模型 接收一批提示词（prompts），生成回应（responses）。
准备 (Preparation): 生成的回应会交由Critic模型（评估状态价值）、奖励模型(RM)（计算即时奖励）和参考模型(Reference Model)（计算与初始模型的KL散度，防止策略偏离太远）进行评估。
训练 (Training): 利用步骤2中计算出的优势函数和损失，更新Actor和Critic模型的参数。