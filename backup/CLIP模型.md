# CLIP


## CLIP训练
CLIP（Contrastive Language–Image Pre-training）的训练方式非常巧妙，其核心是对比学习（Contrastive Learning）。

训练数据：海量的（图像，文本）对。例如，从网上爬取的4亿个图片及其对应的描述文本。
### 训练过程（简化）：

1. 编码：对一个批次（Batch）中的 N 个（图像，文本）对，分别用图像编码器（如ViT）和文本编码器（如Transformer）提取特征，得到 N 个图像向量（I1, I2, ..., In）和 N 个文本向量（T1, T2, ..., Tn）。
2. 计算相似度：计算所有图像向量和所有文本向量之间的余弦相似度，形成一个 N x N 的相似度矩阵。理想情况下，对角线上的元素（I1-T1, I2-T2, ...）是匹配的（正样本），它们的相似度应该最高；非对角线上的元素是不匹配的（负样本），相似度应该最低。
3. 定义损失函数：训练目标就是让这个矩阵的对角线上的相似度尽可能高，非对角线上的相似度尽可能低。
- 从图像角度看，对于图像I1，文本T1应该是正样本，T2, T3,...,Tn都是负样本。
- 从文本角度看，对于文本T1，图像I1应该是正样本，I2, I3,...,In都是负样本。
4. 反向传播：通过计算两个对比损失（Image→Text 和 Text→Image 的交叉熵损失）并反向传播，同时更新图像编码器和文本编码器的参数。

**最终目标**：学习一个共享的 multimodal 特征空间，在这个空间里，对应的图像和文本表示非常接近，而不相关的图像和文本则相距甚远。

### 参数
**temperature作用**：Temperature（温度）是一个在深度学习中非常常见的超参数，尤其在生成模型和softmax函数中，它控制着模型输出的随机性（多样性）和确定性。
为0的时候，贪婪搜索，选择最可能的那个选项（作用：会放大logits之间的差异。最大的logits对应的概率会趋近于1，其他概率趋近于0。）；
大于1，会缩小logits之间的差异。概率分布变得更加“平滑”或“均匀”。
Temperature = 1，标准softmax函数，不进行任何缩放。
