<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>kaikai的博客</title><link>https://naphjohn.github.io</link><description>记录深度学习，大模型中技术感悟</description><copyright>kaikai的博客</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://avatars.githubusercontent.com/u/46772304?v=4</url><title>avatar</title><link>https://naphjohn.github.io</link></image><lastBuildDate>Sat, 06 Sep 2025 13:36:52 +0000</lastBuildDate><managingEditor>kaikai的博客</managingEditor><ttl>60</ttl><webMaster>kaikai的博客</webMaster><item><title>推理加速算法</title><link>https://naphjohn.github.io/post/tui-li-jia-su-suan-fa.html</link><description>**量化：**
量化过程包含缩放（Scale） 和有时需要的零点（Zero Point），以将浮点数范围映射到整数范围。</description><guid isPermaLink="true">https://naphjohn.github.io/post/tui-li-jia-su-suan-fa.html</guid><pubDate>Sat, 06 Sep 2025 13:36:25 +0000</pubDate></item><item><title>训练</title><link>https://naphjohn.github.io/post/xun-lian.html</link><description>### 强化学习中一些基本概念：

**人类反馈强化学习(Reinforcement Learning from Human Feedback, RLHF)**：
核心思想:让LLM的输出能够符合人类偏好(即人类反馈),主要通过人类偏好数据集(例如针对一个response,会
有人类手工标注对其打分)训练一个奖励模型,通过奖励模型计算奖励,以此来判断LLM的输出是否符合人类偏好。</description><guid isPermaLink="true">https://naphjohn.github.io/post/xun-lian.html</guid><pubDate>Sat, 06 Sep 2025 13:21:04 +0000</pubDate></item></channel></rss>