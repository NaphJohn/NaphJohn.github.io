<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    
    <link rel="icon" href="https://avatars.githubusercontent.com/u/46772304?v=4"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="# 大模型优化方式包含以下内容

### 量化
量化过程包含缩放（Scale） 和有时需要的零点（Zero Point），以将浮点数范围映射到整数范围。">
<meta property="og:title" content="大模型推理加速算法">
<meta property="og:description" content="# 大模型优化方式包含以下内容

### 量化
量化过程包含缩放（Scale） 和有时需要的零点（Zero Point），以将浮点数范围映射到整数范围。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://naphjohn.github.io/post/da-mo-xing-tui-li-jia-su-suan-fa.html">
<meta property="og:image" content="https://avatars.githubusercontent.com/u/46772304?v=4">
<title>大模型推理加速算法</title>



</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}

</style>




<body>
    <div id="header">
<h1 class="postTitle">大模型推理加速算法</h1>
<div class="title-right">
    <a href="https://naphjohn.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/NaphJohn/naphjohn.github.io/issues/2" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><h1>大模型优化方式包含以下内容</h1>
<h3>量化</h3>
<p>量化过程包含缩放（Scale） 和有时需要的零点（Zero Point），以将浮点数范围映射到整数范围。</p>
<details><summary>Details</summary>
<p>
包含w8a8,w4a16,w8a16,w8a8c8(perchannel以及pertoken)
一般像w8a8都是激活层pertoken，权重层perchannel量化；一般使用对称量化，非对称对硬件都有要求，计算会变慢，但是精度提升。量化同时降低首token时延和增量推理时延。
</p><p>量化中会出现的一类问题是幻觉问题，可以通过量化回退解决</p>
<p>优势：<br>
模型体积减少：4bit量化可使模型大小减少75%（例如，70B模型从140GB降至35GB）。<br>
降低内存带宽压力：读取4bit权重所需带宽是读取16bit权重的1/4。<br>
加速计算：许多硬件（如GPU的Tensor Cores）对低精度计算有专门优化，计算速度更快。</p>
<p>量化中计算参考文章：<a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization" rel="nofollow">https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization</a></p>
<p></p>
</details>
<h3>框架测优化</h3>
<p>比如PD分离；<br>
比如model和后处理异步化，来提升吞吐；<br>
针对embedding模型只进行全量推理的特点，跳过kv cache相关tensor等的计算；<br>
针对离线大批量处理场景，优化非必要cpu操作耗时，如array转list、list转tuple等；</p>
<h3>分布式优化（并行化）</h3>
<p>比如TP，DP并行化，有效利用多个设备进行推理，从而提升吞吐，降低延迟。</p>
<h3>算子融合</h3>
<p>-fusedDispatch和fusedCombine   （新的AI矢量绕过延迟敏感的SDMA路径，替换All-to-All，改善延迟）<br>
-flashattention （算子融合的一种，将matmul，softmax，mask等融合）</p>
<ul>
<li>比如Operator Fusion 算子融合<br>
通过转换工具，将神经网络中的部分层进行融合，从而降低计算量和数据量，提高推理性能。<br>
常见的两个框架是 FasterTransformer（TensorRT） 和 Deepseed Inference，都可以通过转换的方式，融合 Transformer 的特定层。</li>
</ul>
<h3>显存优化</h3>
<p>压缩kvcache</p>
<ol>
<li>kvcache低精度方式<br>
对 KV—Cache 进行 Int8量化，从而将 KV-Cache 的大小缩小30%-40%。</li>
<li>缓存优化- lru缓存<br>
已有的样本字符不需要再计算，直接返回，比如vllm/entrypoints/chat_utils.py中  _detect_content_format中增加@lru_cache(maxsize=128)</li>
<li><strong>- page attention</strong><br>
原理：通过<strong>逻辑 block 与物理 block</strong> 的映射，实现内存上的非连续存储（映射关系由 block table 维护）。</li>
</ol>
<p>与传统的注意力算法不同，PagedAttention 允许在不连续的内存空间中存储连续的键和值。具体来说，PagedAttention 将每个序列的 KV 缓存划分为多个块，每个块包含固定数量的令牌的键和值。在注意力计算过程中，PagedAttention 内核会有效地识别和获取这些块。由于块不需要在内存中是连续的，因此我们可以像在操作系统的虚拟内存中一样以更灵活的方式管理键和值：可以将块视为页面，将 token 视为字节，将序列视为进程。序列的连续逻辑块通过块表映射到非连续的物理块。随着新 token 的生成，物理区块会按需分配。</p>
<p>创新点：<br>
KV Cache 被切分为固定大小的 block；通过逻辑 block 与物理 block 的映射，实现非连续存储（映射关系由 block table 维护）；PagedAttention 支持按需分配和回收 block，并允许多个序列共享 block。PagedAttention 使用了 copy-on-write（CoW）机制，允许多个生成样本共享大部分输入 prompt 的 KV Cache</p>
<p><strong>（Reference Counting）引用计数</strong>： 跟踪内存块的共享状态，实现安全回收。<br>
<strong>（Block-level Copy-on-Write）优化块复制操作</strong>：只有在真正要写入（修改）的时候，才会进行复制操作，这样不会污染数据<br>
4. KV-Cache 共用<br>
比如MQA、GQA</p>
<h3>flash attention</h3>
<p>原理：针对全局内存和共享存储的 I/O 速 度的不同，尽可能的避免 HBM 中读取或写入注意力矩阵。FlashAttention 目标是尽可能高效地使 用 SRAM 来加快计算速度，避免从全局内存中读取和写入注意力矩阵。</p>
<details><summary>Details</summary>
<p>
Flash Attention 的核心思想是 “分块计算” 和 “核融合”。
*分块（Tiling）*：将大的 Q, K, V 矩阵分割成小的块（Tiles），这些块的大小足以被加载到 GPU 的高速 SRAM 中。
循环计算：通过双重循环，将这些小块从 HBM 加载到 SRAM，然后在 SRAM 内部进行所有的计算步骤（矩阵乘法、Softmax、矩阵乘法）。
*核融合（Kernel Fusion）*：将整个注意力计算（MatMul -&gt; Softmax -&gt; MatMul）融合打包成一个单独的Kernel来方便快速计算
</p><blockquote>
<p>[!CAUTION]</p>
</blockquote>
<p><em>重计算（Recomputation）：</em><br>
Softmax 操作本身是全局的，因为它需要知道所有元素的值来计算归一化分母。在反向传播时，Flash Attention 不需要存储巨大的中间矩阵 S 和 P。它只存储了最终的输出 O 和统计量 l, m。当需要计算梯度时，它会利用存储的 Q, K, V, O, l, m 以及反向传播算法，重新计算出注意力矩阵 S 和 P 的块。这是一种用计算换空间的典型策略。</p>
<p>优势：显存占用从O(N²)降至O(N)，允许处理极长序列。</p>
<p></p>
</details>
<p><strong>flashattenton2:</strong><br>
从GPU硬件特性（并行度、内存层次、Warp调度）出发，重构了算法实现。<br>
最重要的是并行化策略,增加了序列来增加并行</p>
<details><summary>Details</summary>
<p>
FlashAttention-1：它的并行化主要集中在非序列维度上，即批量大小（Batch Size）和注意力头数（Heads）。对于序列本身，它采用的是串行循环的方式。flashattenton2新增在序列长度维度并行。将不同块的计算分配给不同的GPU线程块（Thread Block）。
循环顺序（外循环 vs 内循环）：减少了HBM访问,q变成只要读一遍
Warpanize 设计：每个Warp独立负责计算一个子块的注意力输出，最后再通过共享内存（Shared Memory）进行归约合并。
</p>
</details>
<p><strong>算子融合</strong><br>
原理：减少内核启动开销和内存读写，提升计算效率。</p>
<details><summary>Details</summary>
<p>
神经网络由许多层（算子）组成，如：Linear -&gt; GeLU -&gt; Linear。
在未优化的情况下，每个算子都会启动一个独立的GPU内核（Kernel），并将中间结果写回HBM，然后再由下一个算子读入。这个过程会产生大量的内核启动开销和内存读写延迟。
算子融合将多个连续的操作合并成一个单一的、定制化的内核。
例如：将 MatMul + Add（偏置） + GeLU 三个算子融合成一个 FusedMatMulBiasGeLU 内核。
</p>
</details>
<h3>Continuous Batching (连续批处理)</h3>
<p>核心思想是：以每次前向传播（iteration）为调度单位，而不是以整个请求（sequence）为单位。<br>
也就是已完成请求立即离开批次，新请求立即加入，批处理持续进行。</p>
<ul>
<li>静态批处理 (Static Batching)：在推理前，将多个请求合并为一个大的请求，然后一次性推理。这种方式可以提高吞吐量，但是需要所有请求都完成后才能返回结果，所以一般不会应用</li>
<li>动态批处理 (Dynamic Batching)<br>
收集一批_同时到达_的请求，一起处理，完成后下一批。</li>
</ul>
<h3>其它方式</h3>
<p>比如NZ优化，tensor.cpu同步消除</p>
<p><strong>投机推理（MTP）</strong><br>
本质是大模型一次稍贵但便宜得多的并行计算，再加上K次小模型的廉价计算</p>
<details><summary>Details</summary>
<p>
小模型选择：
优先是
</p><ol>
<li>同架构缩小版（最常用、最有效）：</li>
<li>量化后模型</li>
<li>使用网络前几层（缺点：实现复杂，需要修改模型结构，浅层表示可能无法很好地模拟完整模型的输出。）</li>
<li>专用知识蒸馏小模型（缺点：需要额外的训练成本和数据。）</li>
</ol>
<p>降低延迟的核心原因在于：用一次昂贵的并行计算，换取了多次昂贵的串行计算。因为大模型在这个过程中并没有自己生成任何新token！它只是在计算一组已知候选token的概率。</p>
<p>为什么一次并行验证比三次串行生成快得多？<br>
硬件利用效率：GPU是一种大规模并行处理器，它最擅长做的事情就是一次性处理一大块数据。一次处理长为 L+K 的序列，其计算效率远高于串行处理3个长度分别为 L, L+1, L+2 的序列。<br>
内核启动开销：每次启动GPU内核（Kernel）进行前向传播都有固定的开销。合并成一次内核启动，就消除了两次额外的开销。<br>
内存读写优化：一次处理长序列的数据可以更好地利用缓存，减少与慢速显存（HBM）的通信次数。</p>
<p></p>
</details>
<h3>算子测下发优化</h3>
<ul>
<li>
<p>通信-通信-计算 三流并发<br>
现有的多机多卡分布式moe推理场景下，激活集合通信，门控权重与路由专家、共享专家计算等步骤是顺序下发，导致集群利用率低，推理时延大。</p>
</li>
<li>
<p>优化思路：</p>
<ul>
<li>将共享专家与门控权重计算从全激活改为DP（动态规划）计算。</li>
<li>把原先有依赖关系的计算通信转换为可以并行执行的过程。</li>
<li>利用两种形态的通信（AIV, SDMA），实现“通信-通信-计算”的并发。</li>
</ul>
</li>
<li>
<p>收益点：</p>
<ol>
<li><strong>提升带宽</strong>：通过两种形态通信并行，提高有效带宽。</li>
<li><strong>减少数据量</strong>：先量化再进行通信，使通信的数据量减半。</li>
<li><strong>节省时间</strong>：通信与计算并行，缩短总耗时。</li>
<li><strong>降低计算量</strong>：共享专家与门控权重计算量并行后，整体计算量降至原来的1/DP倍数。</li>
</ol>
</li>
<li>
<p>bert类模型参数量非常小，当计算量较小时，算子存在下发瓶颈，因此针对gelu算子、add_layer_norm算子做下发优化，针对add_layer_norm做二维输入的支持；</p>
</li>
<li>
<p>算子编译优化prune hidden states提前至attention算子之前，减少最后一层计算量；</p>
</li>
</ul>
<details><summary>Details</summary>
<p>
当前最后一层流程：
norm -&gt; qkv_proj -&gt; rope -&gt; attention -&gt; o_proj -&gt; norm -&gt; gate_up_proj -&gt; gelu -&gt; down_proj -&gt; _prune_hidden_states
将prune前移到attention之前：
norm -&gt; qkv_proj -&gt; rope -&gt; _prune_hidden_states -&gt; attention -&gt; o_proj -&gt; norm -&gt; gate_up_proj -&gt; gelu -&gt; down_proj
</p>
</details>
<h2>LLM中一些优化特性</h2>
<p><strong>prefix-caching</strong><br>
PageAttention 主要管理同一个请求内部的 KV Cache，prefix caching能够共享相同提示词的多个请求</p>
<p><strong>PD分离</strong><br>
p与d分开在不同的实例并行进行推理，收益点：<br>
p不会阻塞d，在SLO要求下有收益；<br>
p与d按其计算特点使用不用的并行方式</p>
<p><strong>图模式</strong><br>
将模型的计算过程预先编译成一个静态的计算图，减少host耗时，减少device空闲</p>
<ul>
<li>降低内核启动开销：减少 GPU 内核启动次数。</li>
<li>内存优化：图编译器可以更好地规划内存分配和复用。</li>
<li>算子融合：将多个小算子融合成一个大算子，减少数据搬运，提升计算效率。</li>
</ul>
<p><strong>chunked-prefill</strong><br>
将长输入切分为多个chunk处理，降低显存开销，支持更长序列；<br>
decode不会被prefill完全阻塞，降低decode时延；平衡硬件的资源利用率，提升总体吞吐</p>
<p>其他重要特性</p>
<ul>
<li>Guided-decoding:
<ul>
<li>通过logit-process, 格式化控制模型输出</li>
</ul>
</li>
<li>Reasoning content:
<ul>
<li>解析返回体中的reasoning</li>
</ul>
</li>
<li>Function call:
<ul>
<li>解析返回体中的tool；通常和guided-decoding一起使用</li>
</ul>
</li>
<li>Multi-lora:
<ul>
<li>基模型和多个挂载的lora权重一起推理服务</li>
</ul>
</li>
</ul></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright © <span id="copyrightYear"></span> <a href="https://naphjohn.github.io">kaikai的博客</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if(""!=""){
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","NaphJohn/naphjohn.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}



</script>


</html>
